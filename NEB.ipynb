{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchinfo import summary         \n",
    "import numpy as np\n",
    "import torchvision.transforms as tf\n",
    "import transformers                    \n",
    "from tensorboardX import SummaryWriter \n",
    "from pkg_resources import packaging    \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from openTSNE import TSNE              \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('paras.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_by_category(dataset, category):\n",
    "    category_indices = [i for i, label in enumerate(dataset.targets) if label == category]\n",
    "    images = [dataset[i][0] for i in category_indices]\n",
    "    return images\n",
    "\n",
    "# Revert the transformations in reverse order\n",
    "reverted_tf = tf.Compose([\n",
    "    tf.Normalize((-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225), (1 / 0.229, 1 / 0.224, 1 / 0.225)),\n",
    "    tf.Normalize((0, 0, 0), (1, 1, 1)),  # Unnormalize the tensor\n",
    "    tf.ConvertImageDtype(torch.uint8),\n",
    "    tf.ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_images = get_images_by_category(test_set, 5)  # \"dog\" category label is 5\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(reverted_tf(dog_images[i]))\n",
    "    plt.axis('off')\n",
    "    plt.title('Dog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEB(image1, image2, target_index, intermediate_no=10, model=model, mode='standard', k=0.5, max_itr=1000):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # fc_layer = model.fc\n",
    "    # model_without_fc = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    image1 = image1.to(device)\n",
    "    image1_batch = image1.unsqueeze(0)\n",
    "    output1 = model(image1_batch)\n",
    "    image2 = image2.to(device)\n",
    "    image2_batch = image2.unsqueeze(0)\n",
    "    output2 = model(image2_batch)\n",
    "    spacing = (image2_batch - image1_batch) / (intermediate_no + 1)\n",
    "    intermediate_images = torch.stack([image1_batch + spacing * i for i in range(1, intermediate_no + 1)])\n",
    "\n",
    "    for _ in range(max_itr):\n",
    "        f_total = 0\n",
    "        inputs, outputs, potential_grad = [], [], []\n",
    "        for i in range(intermediate_no):\n",
    "            inputs.append(torch.tensor(intermediate_images[i].clone(), requires_grad=True))\n",
    "            outputs.append(model(intermediate_images))\n",
    "            outputs[-1].backward()\n",
    "            with torch.no_grad():\n",
    "                potential_grad.append(inputs[-1].grad.copy())\n",
    "        direction = get_direction(inputs, outputs, target_index)\n",
    "        for i in range(intermediate_no):\n",
    "            f_potential = -potential_grad[i] + potential_grad @ direction[i] * direction[i]\n",
    "            f_spring = k * (torch.abs(inputs[i+1] - inputs[i]) - torch.abs(inputs[i] - inputs[i-1])) * direction[i]\n",
    "            f = f_potential + f_spring\n",
    "            f_total += f\n",
    "            with torch.no_grad():\n",
    "                inputs[i] += lr * f\n",
    "        if f_total < 0.01:\n",
    "            break\n",
    "\n",
    "    return intermediate_images\n",
    "\n",
    "def get_direction(inputs, outputs, target_index):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEB(image1, image2, target_index, intermediate_no=10, model=model, mode='standard', k=0.5, max_itr=1000):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    fc_layer = model.fc\n",
    "    model_without_fc = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    image1 = image1.to(device)\n",
    "    image1_batch = image1.unsqueeze(0)\n",
    "    output1 = model_without_fc(image1_batch)\n",
    "    image2 = image2.to(device)\n",
    "    image2_batch = image2.unsqueeze(0)\n",
    "    output2 = model_without_fc(image2_batch)\n",
    "    spacing = (output2 - output1) / (intermediate_no + 1)\n",
    "    intermediate_images = torch.stack([output1 + spacing * i for i in range(1, intermediate_no + 1)])\n",
    "\n",
    "    intermediate_images = NEB_train(intermediate_images, fc_layer, target_index, intermediate_no, k=k, max_itr=max_itr)\n",
    "\n",
    "    return intermediate_images\n",
    "\n",
    "def NEB_train(intermediate_images, fc_layer, target_index, intermediate_no, k, max_itr, tol):\n",
    "\n",
    "    for _ in range(max_itr):\n",
    "        f_total = 0\n",
    "        inputs, outputs, potential_grad = [], [], []\n",
    "        for i in range(intermediate_no):\n",
    "            inputs.append(torch.tensor(intermediate_images[i].clone(), requires_grad=True))\n",
    "            outputs.append(fc_layer(intermediate_images))\n",
    "            outputs[-1].backward()\n",
    "            with torch.no_grad():\n",
    "                potential_grad.append(inputs[-1].grad.copy())\n",
    "        direction = get_direction(inputs, outputs, target_index)\n",
    "        for i in range(intermediate_no):\n",
    "            f_potential = -potential_grad[i] + potential_grad @ direction[i] * direction[i]\n",
    "            f_spring = k * (torch.abs(inputs[i+1] - inputs[i]) - torch.abs(inputs[i] - inputs[i-1])) * direction[i]\n",
    "            f = f_potential + f_spring\n",
    "            f_total += f\n",
    "            with torch.no_grad():\n",
    "                inputs[i] += lr * f\n",
    "        if f_total < tol:\n",
    "            break\n",
    "\n",
    "def get_direction(inputs, outputs, target_index):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0, 1000, size=2)\n",
    "image1, image2 = dog_images[indices[0]], dog_images[indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_images = NEB(image1, image2, intermediate_no=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def blur_layer(x, kernel_size=3, sigma=1.0):\n",
    "    padding = kernel_size // 2\n",
    "    channels = x.size(1)\n",
    "\n",
    "    kernel = torch.tensor([[1, 2, 1],\n",
    "                           [2, 4, 2],\n",
    "                           [1, 2, 1]], dtype=torch.float32) / 16.0  # Gaussian blur kernel\n",
    "\n",
    "    kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    kernel = kernel.expand(channels, -1, -1, -1).to(x.device)\n",
    "\n",
    "    blurred = F.conv2d(x, kernel, padding=padding, groups=channels)\n",
    "\n",
    "    return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0, 1000, size=2)\n",
    "image1 = dog_images[indices[0]]\n",
    "\n",
    "image_size = (3, 224, 224)  # Size of the image tensor\n",
    "image2 = torch.randn(image_size)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image2 = normalize(image2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pre-trained ResNet model and extract the model without the FC layer\n",
    "model_without_fc = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "transform_gaussion = transforms.GaussianBlur(21,10)\n",
    "\n",
    "image1 = image1.to(device)\n",
    "image2 = image2.to(device)\n",
    "image1 = image1.unsqueeze(0)  # Assuming image1 is your input image tensor\n",
    "image2 = image2.unsqueeze(0)  # Assuming image2 is your target image tensor\n",
    "\n",
    "# Create a tensor to hold the optimized image1\n",
    "image1_optimized = torch.tensor(image2.clone(), requires_grad=True)\n",
    "\n",
    "# Set up the optimizer\n",
    "# optimizer = Adam([image1_optimized], lr=0.01)\n",
    "\n",
    "# Optimization loop\n",
    "num_iterations = 1000\n",
    "losses = []\n",
    "for _ in range(num_iterations):\n",
    "    # Forward pass with image1_optimized\n",
    "\n",
    "    image1_optimized = torch.tensor(transform_gaussion(image1_optimized).clone(), requires_grad=True)\n",
    "    optimizer = Adam([image1_optimized], lr=0.01)\n",
    "    output1 = model_without_fc(image1_optimized)\n",
    "\n",
    "    # Forward pass with image2\n",
    "    output2 = model_without_fc(image1)\n",
    "\n",
    "    # Compute the loss between output1 and output2\n",
    "    loss = torch.mean((output1 - output2) ** 2)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients of the loss with respect to image1_optimized\n",
    "    loss.backward()\n",
    "\n",
    "    # Update image1_optimized\n",
    "    optimizer.step()\n",
    "    if loss < 0.01:\n",
    "        break\n",
    "\n",
    "# Access the optimized image1\n",
    "optimized_image1 = image1_optimized.squeeze()\n",
    "print(losses[0], losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(reverted_tf(image1.squeeze()))\n",
    "plt.axis('off')\n",
    "plt.title('Dog')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reverted_tf(optimized_image1))\n",
    "plt.axis('off')\n",
    "plt.title('Dog')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([tensor.cpu().detach().numpy() for tensor in losses])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
